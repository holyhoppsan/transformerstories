{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan's todo list for tomorrow:\n",
    "\n",
    "* Setup ML Flow for testing\n",
    "* Look into whether Argmax is not the right choice\n",
    "* Save progress after each epoch\n",
    "* Train on bigger datasets - UPDATE, this does not seem to make a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0a - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl (24.2 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, urllib3, tqdm, pyyaml, pyarrow-hotfix, pyarrow, multidict, idna, fsspec, frozenlist, dill, charset-normalizer, certifi, attrs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 idna-3.6 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 requests-2.31.0 sentencepiece-0.2.0 tqdm-4.66.2 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install datasets sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0b - Import module dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on: cpu\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_output_dir = 'sentencepiece_models'\n",
    "sentencepiece_corpus_filename = f\"tiny_stories_texts.txt\"\n",
    "sentencepiece_model_prefix = os.path.join(sentencepiece_output_dir, 'tiny_stories_spm_sampled')\n",
    "\n",
    "story_token_max_length = 20\n",
    "\n",
    "use_small_dataset = True\n",
    "small_data_set_size = 10000\n",
    "\n",
    "vocabulary_size = 8000\n",
    "\n",
    "embedding_size = 256\n",
    "num_decoder_layers = 6\n",
    "num_heads = 8\n",
    "forward_layer_expansion = 4\n",
    "dropout = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running models on: {device}\")\n",
    "\n",
    "inference_debug_log_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Load datasets\n",
    "\n",
    "Read the tiny stories data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stories set size (Pre resize): 2119719\n",
      "Validation stories set size (Pre resize): 21990\n",
      "Using small datasets\n",
      "Training stories set size: 10000\n",
      "Validation stories set size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load the Tiny Stories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "\n",
    "train_stories = train_dataset['text']\n",
    "valid_stories = valid_dataset['text']\n",
    "\n",
    "print(f\"Training stories set size (Pre resize): {len(train_stories)}\")\n",
    "print(f\"Validation stories set size (Pre resize): {len(valid_stories)}\")\n",
    "\n",
    "if use_small_dataset is True:\n",
    "    print(\"Using small datasets\")\n",
    "    train_stories = train_stories[:small_data_set_size]\n",
    "    valid_stories = valid_stories[:small_data_set_size]\n",
    "\n",
    "print(f\"Training stories set size: {len(train_stories)}\")\n",
    "print(f\"Validation stories set size: {len(valid_stories)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all of the data set and export it to a text file for training of the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the files\n",
    "if not os.path.exists(sentencepiece_output_dir):\n",
    "    os.makedirs(sentencepiece_output_dir)\n",
    "\n",
    "# Save all texts to a single file in the specified directory, one story per line\n",
    "sentencepiece_corpus_file_path = os.path.join(sentencepiece_output_dir, sentencepiece_corpus_filename)\n",
    "\n",
    "\n",
    "# Combine texts from training and validation sets\n",
    "all_texts = train_dataset['text'] + valid_dataset['text']\n",
    "\n",
    "random.shuffle(all_texts)\n",
    "\n",
    "# Sample a smaller subset of the dataset, e.g., 10% of the data\n",
    "sample_size = int(0.1 * len(all_texts))\n",
    "sampled_text = all_texts[:sample_size]\n",
    "\n",
    "# Save all texts to a single file, one story per line\n",
    "with open(sentencepiece_corpus_file_path, 'w', encoding='utf-8') as f:\n",
    "    for story in sampled_text:\n",
    "        f.write(story + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next generate the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sentencepiece_models/tiny_stories_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_models/tiny_stories_spm_sampled\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sentencepiece_models/tiny_stories_texts.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1154199), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1154199 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=191010460\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9591% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=56\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999591\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1152948 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=115341415\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 73328 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1152948\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 105996\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 105996 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35340 obj=9.44924 num_tokens=251879 num_tokens/piece=7.12731\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25521 obj=7.28078 num_tokens=253637 num_tokens/piece=9.93836\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19140 obj=7.20601 num_tokens=260830 num_tokens/piece=13.6275\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19134 obj=7.20224 num_tokens=260905 num_tokens/piece=13.6357\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14350 obj=7.20779 num_tokens=278479 num_tokens/piece=19.4062\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14350 obj=7.20466 num_tokens=278434 num_tokens/piece=19.4031\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10762 obj=7.21591 num_tokens=300012 num_tokens/piece=27.877\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10762 obj=7.21393 num_tokens=299954 num_tokens/piece=27.8716\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=7.22731 num_tokens=315830 num_tokens/piece=35.8898\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=7.22534 num_tokens=315814 num_tokens/piece=35.888\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sentencepiece_models/tiny_stories_spm_sampled.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sentencepiece_models/tiny_stories_spm_sampled.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=sentencepiece_corpus_file_path, model_prefix=sentencepiece_model_prefix, vocab_size=vocabulary_size, character_coverage=0.9995, model_type='unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next initialize the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model_path = f\"{sentencepiece_model_prefix}.model\"\n",
    "sp = spm.SentencePieceProcessor(model_file=spm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Generate the input data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 50, 26, 5, 8, 38, 58, 79, 24, 123, 8, 1901, 21, 13, 199, 3, 12, 167, 10, 9]\n",
      "[50, 26, 5, 8, 38, 58, 79, 24, 123, 8, 1901, 21, 13, 199, 3, 12, 167, 10, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(stories, sp, max_length):\n",
    "    inputs, labels = [], []\n",
    "    bos_id, eos_id = sp.bos_id(), sp.eos_id()\n",
    "    \n",
    "    for story in stories:\n",
    "        # Tokenize the story and truncate if necessary\n",
    "        tokens = sp.encode(story, out_type=int)[:max_length -1]\n",
    "\n",
    "        # Prepend BOS and append EOS token IDs\n",
    "        input_ids = [bos_id] + tokens\n",
    "        label_ids = tokens + [eos_id]  # Adjusted to ensure labels also start with bos_id and end with eos_id\n",
    "\n",
    "        # Ensure the final lists are of max_length\n",
    "        # This might already be ensured by previous steps, but double-checking to align with the assertion requirements\n",
    "        input_ids = (input_ids + [eos_id] * max_length)[:max_length]  # Padding with eos_id if necessary, though this should be rare given earlier truncation\n",
    "        label_ids = (label_ids + [eos_id] * max_length)[:max_length]\n",
    "\n",
    "        # Assertions to ensure each sequence meets the specified criteria\n",
    "        assert len(input_ids) == max_length, f\"Input sequence length does not match max_length. Length: {len(input_ids)}\"\n",
    "        assert len(label_ids) == max_length, f\"Label sequence length does not match max_length. Length: {len(label_ids)}\"\n",
    "        assert input_ids[0] == bos_id, \"Input sequence does not start with bos_id.\"\n",
    "        assert label_ids[-1] == eos_id, \"Label sequence does not end with eos_id.\"\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "def assert_max_length(data, max_length):\n",
    "    for entry in data:\n",
    "        # Each entry should not exceed max_length tokens\n",
    "        assert len(entry) <= max_length, f\"Entry exceeds max_length of {max_length} tokens.\"\n",
    "\n",
    "\n",
    "train_inputs, train_labels = prepare_data(train_stories, sp, story_token_max_length)\n",
    "assert(len(train_inputs) == len(train_stories))\n",
    "assert_max_length(train_inputs, story_token_max_length)\n",
    "assert_max_length(train_labels, story_token_max_length)\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "\n",
    "valid_inputs, valid_labels = prepare_data(valid_stories, sp, story_token_max_length)\n",
    "assert(len(valid_inputs) == len(valid_stories))\n",
    "assert_max_length(valid_inputs, story_token_max_length)\n",
    "assert_max_length(valid_labels, story_token_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads = heads, dropout = dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion* embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        attention_output, _ = self.attention(x, x, x, attn_mask=src_mask)\n",
    "        x = self.dropout(self.norm1(attention_output + x))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_size, heads, forward_expansion, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fully_connected_layer_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        out = self.fully_connected_layer_out(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,847,680 trainable parameters\n",
      "Epoch 1, Train Loss: 2.9769, Validation Loss: 2.3731\n",
      "Text generated after Epoch 1: The ancient house , there was a little girl named Lily . She loved to play with her and loved\n",
      "Epoch 2, Train Loss: 2.2877, Validation Loss: 2.2454\n",
      "Text generated after Epoch 2: The ancient house , there was a little girl named Lily . She loved little girl named Lily . One\n",
      "Epoch 3, Train Loss: 2.1558, Validation Loss: 2.2004\n",
      "Text generated after Epoch 3: The ancient house , there was a little girl named . She was a little and loved to play with\n",
      "Epoch 4, Train Loss: 2.0751, Validation Loss: 2.1956\n",
      "Text generated after Epoch 4: The ancient house , there was a little girl . She loved to play with his to play with his\n",
      "Epoch 5, Train Loss: 2.0109, Validation Loss: 2.2001\n",
      "Text generated after Epoch 5: The ancient house and Ben . She was a little girl named Lily . She loved to play with his\n",
      "Epoch 6, Train Loss: 1.9559, Validation Loss: 2.1867\n",
      "Text generated after Epoch 6: The ancient house and play . She loved to play in the park . She loved to play with her\n",
      "Epoch 7, Train Loss: 1.9071, Validation Loss: 2.2011\n",
      "Text generated after Epoch 7: The ancient house and Ben . She was a little girl named Lily . She loved to play with her\n",
      "Epoch 8, Train Loss: 1.8666, Validation Loss: 2.2199\n",
      "Text generated after Epoch 8: The ancient house . She was a little girl named . She was a little and loved to play with\n",
      "Epoch 9, Train Loss: 1.8322, Validation Loss: 2.2254\n",
      "Text generated after Epoch 9: The ancient house . She loved to play outside in the park . She was a big and he was\n",
      "Epoch 10, Train Loss: 1.8035, Validation Loss: 2.2323\n",
      "Text generated after Epoch 10: The ancient house . She was a little the park . She was a little and she was a .\n",
      "Epoch 11, Train Loss: 3.6302, Validation Loss: 4.8624\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 11: The ancient house a a a\n",
      "Epoch 12, Train Loss: 4.8740, Validation Loss: 4.8499\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 12: The ancient house a a a\n",
      "Epoch 13, Train Loss: 4.8491, Validation Loss: 4.8002\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 13: The ancient house a a a\n",
      "Epoch 14, Train Loss: 4.7760, Validation Loss: 4.8439\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 14: The ancient house a a a\n",
      "Epoch 15, Train Loss: 4.8574, Validation Loss: 4.8467\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 15: The ancient house a a a\n",
      "Epoch 16, Train Loss: 4.8553, Validation Loss: 4.8453\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 16: The ancient house a a a\n",
      "Epoch 17, Train Loss: 4.8544, Validation Loss: 4.8455\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 17: The ancient house a a a\n",
      "Epoch 18, Train Loss: 4.8534, Validation Loss: 4.8469\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 18: The ancient house a a a\n",
      "Epoch 19, Train Loss: 4.8534, Validation Loss: 4.8444\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 19: The ancient house a a a\n",
      "Epoch 20, Train Loss: 4.8531, Validation Loss: 4.8475\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 20: The ancient house a a a\n",
      "Epoch 21, Train Loss: 4.8523, Validation Loss: 4.8472\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 21: The ancient house a a a\n",
      "Epoch 22, Train Loss: 4.8516, Validation Loss: 4.8485\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 22: The ancient house a a a\n",
      "Epoch 23, Train Loss: 4.8517, Validation Loss: 4.8508\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 23: The ancient house a a a\n",
      "Epoch 24, Train Loss: 4.8506, Validation Loss: 4.8512\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 24: The ancient house a a a\n",
      "Epoch 25, Train Loss: 4.8499, Validation Loss: 4.8506\n",
      "Stopping early due to repeated token (8) detected 3 times in a row.\n",
      "Text generated after Epoch 25: The ancient house a a a\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoder(\n",
    "    vocab_size = vocabulary_size,\n",
    "    embed_size = embedding_size,\n",
    "    num_layers = num_decoder_layers,\n",
    "    heads = num_heads,\n",
    "    device = device,\n",
    "    forward_expansion = forward_layer_expansion,\n",
    "    dropout = dropout,\n",
    "    max_length = story_token_max_length\n",
    ").to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# Assuming `train_inputs` and `train_labels` are your processed datasets\n",
    "train_dataset_processed = TinyStoriesDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset_processed, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset_processed = TinyStoriesDataset(valid_inputs, valid_labels)\n",
    "validation_loader = DataLoader(validation_dataset_processed, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, src_mask=None)\n",
    "            loss = criterion(outputs.transpose(1,2), labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(loader)\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, src_mask = None)\n",
    "            loss = criterion(outputs.transpose(1,2), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = validate(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        generated_text = generate_text_simple(model, \"The ancient house\", sp, device, story_token_max_length)\n",
    "        print(f\"Text generated after Epoch {epoch+1}: {generated_text}\")  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, validation_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - inference methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Timmy . She was a time . She loved to play with her\n"
     ]
    }
   ],
   "source": [
    "repetition_threshold = 3\n",
    "\n",
    "def generate_text_simple(model, start_prompt, sp, device, max_length):\n",
    "    model.eval()\n",
    "    words = start_prompt.split()\n",
    "    token_ids = sp.encode(start_prompt, out_type=int)\n",
    "\n",
    "    if max(token_ids) >= vocabulary_size:\n",
    "        raise ValueError(f\"Token ID {max(token_ids)} exceeds vocab size of {vocabulary_size}\")\n",
    "\n",
    "    consecutive_repetitions = 0\n",
    "    last_token_id = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        input_ids = torch.tensor([token_ids], device=device)\n",
    "\n",
    "        if inference_debug_log_enabled:\n",
    "            print(f\"Input IDs: {input_ids}\")\n",
    "            print(f\"Shape: {input_ids.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, src_mask=None)\n",
    "            predictions = outputs[:, -1, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1).item()\n",
    "\n",
    "        # Check for consecutive repetitions\n",
    "        if predicted_id == last_token_id:\n",
    "            consecutive_repetitions += 1\n",
    "        else:\n",
    "            consecutive_repetitions = 0  # Reset the counter if the current token is different\n",
    "\n",
    "        last_token_id = predicted_id  # Update the last seen token ID\n",
    "\n",
    "        # Exit if the repetition threshold is reached\n",
    "        if consecutive_repetitions >= repetition_threshold:\n",
    "            print(f\"Stopping early due to repeated token ({predicted_id}) detected {repetition_threshold} times in a row.\")\n",
    "            break\n",
    "\n",
    "        if predicted_id == sp.eos_id():\n",
    "            break\n",
    "\n",
    "        token_ids.append(predicted_id)\n",
    "        generated_word = sp.DecodeIds([predicted_id])\n",
    "        words.append(generated_word)\n",
    "\n",
    "    generated_text = ' '.join(words)\n",
    "    return generated_text\n",
    "\n",
    "generated_text = generate_text_simple(model, \"One day, a little girl\", sp, device, story_token_max_length)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debuggin notes\n",
    "\n",
    "## Things to explore\n",
    "\n",
    "1. [Doing inference with strings from the dataset](#Doing-inference-with-strings-from-the-dataset)\n",
    "1. Do I understand my architecture?\n",
    "1. [Is my model too simple? (Under fitting)](#Is-my-model-too-simple?-(Under-fitting))\n",
    "1. [Is my model overfitting?](#Is-my-model-overfitting?)\n",
    "1. [Learning rate optimizations]()\n",
    "1. [Sequence Length Handling]()\n",
    "1. [Repetition Penalty]()\n",
    "\n",
    "\n",
    "## Investigations\n",
    "\n",
    "### Doing inference with strings from the dataset\n",
    "\n",
    "I tried to use the \"One day, a little girl\" string from the first entry in the dataset to see it would perform better with an input string it had already seen. But this didn't do much.\n",
    "\n",
    "Here's the output after:\n",
    "\n",
    "```\n",
    "Epoch 1, Train Loss: 1.1776, Validation Loss: 0.3109\n",
    "Stopping early due to repeated token (180) detected 3 times in a row.\n",
    "Text generated after Epoch 1: The ancient house house house house\n",
    "Epoch 2, Train Loss: 0.1383, Validation Loss: 0.0950\n",
    "Stopping early due to repeated token (180) detected 3 times in a row.\n",
    "Text generated after Epoch 2: The ancient house house house house\n",
    "Epoch 3, Train Loss: 0.0280, Validation Loss: 0.0669\n",
    "Stopping early due to repeated token (180) detected 3 times in a row.\n",
    "Text generated after Epoch 3: The ancient house house house house\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "### Is my model too simple? (Under fitting)\n",
    "\n",
    "TODO: Update with investigation.\n",
    "\n",
    "Notes from chat GPT: \n",
    "\n",
    "`Underfitting: If the model is too simple, it might not have learned the underlying patterns in the data sufficiently. Consider increasing the model complexity by adding more layers or increasing the embedding size.`\n",
    "\n",
    "### Is my model overfitting?\n",
    "\n",
    "#### \n",
    "\n",
    "TODO: Investigate regularization techniques if needed.\n",
    "\n",
    "`Overfitting: If the model has memorized the training data rather than learning to generalize, it may perform poorly on slightly different inputs or validation data. Regularization techniques (e.g., dropout, weight decay) or more training data could help.`\n",
    "\n",
    "#### Examination of training and validation lost.\n",
    "\n",
    "Given the following training output:\n",
    "\n",
    "```\n",
    "Epoch 1, Train Loss: 1.1776, Validation Loss: 0.3109\n",
    "Stopping early due to repeated token (180) detected 3 times in a row.\n",
    "Text generated after Epoch 1: The ancient house house house house\n",
    "Epoch 2, Train Loss: 0.1383, Validation Loss: 0.0950\n",
    "Stopping early due to repeated token (180) detected 3 times in a row.\n",
    "Text generated after Epoch 2: The ancient house house house house\n",
    "Epoch 3, Train Loss: 0.0280, Validation Loss: 0.0669\n",
    "Stopping early due to repeated token (180) detected 3 times in a row.\n",
    "Text generated after Epoch 3: The ancient house house house house\n",
    "Epoch 4, Train Loss: 0.0032, Validation Loss: 0.0638\n",
    "Stopping early due to repeated token (180) detected 3 times in a row.\n",
    "Text generated after Epoch 4: The ancient house house house house\n",
    "Epoch 5, Train Loss: 0.0008, Validation Loss: 0.0648\n",
    "```\n",
    "\n",
    "We can see that the training loss is going down, while the validation loss stagnates, but The problem of predicting the same name over and over from the first epoch already, overfitting is most likely not the issue in the first case.\n",
    "\n",
    "\n",
    "### Learning rate optimizations\n",
    "\n",
    "TODO: investigate\n",
    "\n",
    "From chatGPT\n",
    "```\n",
    "The choice of learning_rate can significantly affect training. Too high a learning rate can cause the model to converge too quickly to a suboptimal solution, while too low a rate can slow down training or cause it to stall.\n",
    "Solution: Consider using a learning rate scheduler to adjust the rate during training. torch.optim.lr_scheduler provides several options, such as StepLR or ReduceLROnPlateau, which can help improve training dynamics.\n",
    "```\n",
    "\n",
    "### Sequence Length Handling\n",
    "\n",
    "From Chatgpt:\n",
    "\n",
    "```\n",
    "The fixed story_token_max_length determines how much context the model considers during training and inference. If this length is not optimally chosen, it could impact the model's ability to generate coherent text.\n",
    "Solution: Experiment with different sequence lengths. Also, ensure that your padding and truncation strategies during preprocessing align with the model's expectations.\n",
    "```\n",
    "\n",
    "### Repetition Penalty\n",
    "\n",
    "From chatgpt\n",
    "\n",
    "```\n",
    "Your method to stop generating text after a certain number of repeated tokens is a practical approach to handle repetitive output. However, this doesn't address the underlying cause of why the model prefers these repetitions.\n",
    "Solution: Consider implementing more nuanced sampling methods during inference, such as top-k or top-p (nucleus) sampling, which can encourage diversity in the generated text.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerstories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
