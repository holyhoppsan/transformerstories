{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0a - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl (24.2 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, urllib3, tqdm, pyyaml, pyarrow-hotfix, pyarrow, multidict, idna, fsspec, frozenlist, dill, charset-normalizer, certifi, attrs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 idna-3.6 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 requests-2.31.0 sentencepiece-0.2.0 tqdm-4.66.2 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install datasets sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0b - Import module dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_output_dir = 'sentencepiece_models'\n",
    "sentencepiece_corpus_filename = f\"tiny_stories_texts.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Load datasets\n",
    "\n",
    "Read the tiny stories data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.02k/1.02k [00:00<00:00, 1.78MB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 249M/249M [00:40<00:00, 6.07MB/s] \n",
      "Downloading data: 100%|██████████| 248M/248M [00:27<00:00, 9.00MB/s] \n",
      "Downloading data: 100%|██████████| 246M/246M [00:21<00:00, 11.6MB/s] \n",
      "Downloading data: 100%|██████████| 248M/248M [00:23<00:00, 10.5MB/s] \n",
      "Downloading data: 100%|██████████| 9.99M/9.99M [00:01<00:00, 9.12MB/s]\n",
      "Generating train split: 2119719 examples [00:02, 717132.82 examples/s]\n",
      "Generating validation split: 21990 examples [00:00, 850651.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the Tiny Stories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all of the data set and export it to a text file for training of the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the files\n",
    "if not os.path.exists(sentencepiece_output_dir):\n",
    "    os.makedirs(sentencepiece_output_dir)\n",
    "\n",
    "# Save all texts to a single file in the specified directory, one story per line\n",
    "sentencepiece_corpus_file_path = os.path.join(sentencepiece_output_dir, sentencepiece_corpus_filename)\n",
    "\n",
    "\n",
    "# Combine texts from training and validation sets\n",
    "all_texts = train_dataset['text'] + valid_dataset['text']\n",
    "\n",
    "random.shuffle(all_texts)\n",
    "\n",
    "# Sample a smaller subset of the dataset, e.g., 10% of the data\n",
    "sample_size = int(0.1 * len(all_texts))\n",
    "sampled_text = all_texts[:sample_size]\n",
    "\n",
    "# Save all texts to a single file, one story per line\n",
    "with open(sentencepiece_corpus_file_path, 'w', encoding='utf-8') as f:\n",
    "    for story in sampled_text:\n",
    "        f.write(story + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next generate the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sentencepiece_models/tiny_stories_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_models/tiny_stories_spm_sampled\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  �� \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sentencepiece_models/tiny_stories_texts.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1155594), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1155594 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=191159844\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9593% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=56\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999594\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1154342 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=115420957\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 73406 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1154342\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 106503\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 106503 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35378 obj=9.44331 num_tokens=253174 num_tokens/piece=7.15626\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25545 obj=7.28255 num_tokens=254881 num_tokens/piece=9.97773\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19158 obj=7.20697 num_tokens=262275 num_tokens/piece=13.6901\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19157 obj=7.20275 num_tokens=262371 num_tokens/piece=13.6958\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14367 obj=7.20719 num_tokens=279978 num_tokens/piece=19.4876\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14367 obj=7.20582 num_tokens=279948 num_tokens/piece=19.4855\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10775 obj=7.21711 num_tokens=301429 num_tokens/piece=27.9748\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10775 obj=7.21518 num_tokens=301368 num_tokens/piece=27.9692\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=7.22849 num_tokens=317874 num_tokens/piece=36.122\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=7.22651 num_tokens=317871 num_tokens/piece=36.1217\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sentencepiece_models/tiny_stories_spm_sampled.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sentencepiece_models/tiny_stories_spm_sampled.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=sentencepiece_corpus_file_path, model_prefix=os.path.join(sentencepiece_output_dir, 'tiny_stories_spm_sampled'), vocab_size=8000, character_coverage=0.9995, model_type='unigram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerstories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
