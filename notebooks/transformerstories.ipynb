{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0a - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl (24.2 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, urllib3, tqdm, pyyaml, pyarrow-hotfix, pyarrow, multidict, idna, fsspec, frozenlist, dill, charset-normalizer, certifi, attrs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 idna-3.6 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 requests-2.31.0 sentencepiece-0.2.0 tqdm-4.66.2 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install datasets sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0b - Import module dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_output_dir = 'sentencepiece_models'\n",
    "sentencepiece_corpus_filename = f\"tiny_stories_texts.txt\"\n",
    "sentencepiece_model_prefix = os.path.join(sentencepiece_output_dir, 'tiny_stories_spm_sampled')\n",
    "\n",
    "story_token_max_length = 20\n",
    "\n",
    "use_small_dataset = True\n",
    "small_data_set_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Load datasets\n",
    "\n",
    "Read the tiny stories data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using small datasets\n",
      "Training stories set size: 10\n",
      "Valisation stories set size: 10\n"
     ]
    }
   ],
   "source": [
    "# Load the Tiny Stories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "\n",
    "train_stories = train_dataset['text']\n",
    "valid_stories = valid_dataset['text']\n",
    "\n",
    "if use_small_dataset is True:\n",
    "    print(\"Using small datasets\")\n",
    "    train_stories = train_stories[:small_data_set_size]\n",
    "    valid_stories = valid_stories[:small_data_set_size]\n",
    "\n",
    "print(f\"Training stories set size: {len(train_stories)}\")\n",
    "print(f\"Valisation stories set size: {len(valid_stories)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all of the data set and export it to a text file for training of the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the files\n",
    "if not os.path.exists(sentencepiece_output_dir):\n",
    "    os.makedirs(sentencepiece_output_dir)\n",
    "\n",
    "# Save all texts to a single file in the specified directory, one story per line\n",
    "sentencepiece_corpus_file_path = os.path.join(sentencepiece_output_dir, sentencepiece_corpus_filename)\n",
    "\n",
    "\n",
    "# Combine texts from training and validation sets\n",
    "all_texts = train_dataset['text'] + valid_dataset['text']\n",
    "\n",
    "random.shuffle(all_texts)\n",
    "\n",
    "# Sample a smaller subset of the dataset, e.g., 10% of the data\n",
    "sample_size = int(0.1 * len(all_texts))\n",
    "sampled_text = all_texts[:sample_size]\n",
    "\n",
    "# Save all texts to a single file, one story per line\n",
    "with open(sentencepiece_corpus_file_path, 'w', encoding='utf-8') as f:\n",
    "    for story in sampled_text:\n",
    "        f.write(story + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next generate the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sentencepiece_models/tiny_stories_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_models/tiny_stories_spm_sampled\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  �� \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sentencepiece_models/tiny_stories_texts.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1155594), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1155594 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=191159844\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9593% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=56\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999594\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1154342 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=115420957\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 73406 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1154342\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 106503\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 106503 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35378 obj=9.44331 num_tokens=253174 num_tokens/piece=7.15626\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25545 obj=7.28255 num_tokens=254881 num_tokens/piece=9.97773\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19158 obj=7.20697 num_tokens=262275 num_tokens/piece=13.6901\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19157 obj=7.20275 num_tokens=262371 num_tokens/piece=13.6958\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14367 obj=7.20719 num_tokens=279978 num_tokens/piece=19.4876\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14367 obj=7.20582 num_tokens=279948 num_tokens/piece=19.4855\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10775 obj=7.21711 num_tokens=301429 num_tokens/piece=27.9748\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10775 obj=7.21518 num_tokens=301368 num_tokens/piece=27.9692\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=7.22849 num_tokens=317874 num_tokens/piece=36.122\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=7.22651 num_tokens=317871 num_tokens/piece=36.1217\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sentencepiece_models/tiny_stories_spm_sampled.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sentencepiece_models/tiny_stories_spm_sampled.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=sentencepiece_corpus_file_path, model_prefix=sentencepiece_model_prefix, vocab_size=8000, character_coverage=0.9995, model_type='unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next initialize the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model_path = f\"{sentencepiece_model_prefix}.model\"\n",
    "sp = spm.SentencePieceProcessor(model_file=spm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Generate the input data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 50, 26, 5, 8, 38, 59, 81, 24, 120, 8, 2001, 21, 13, 198, 3, 12, 168, 10, 2], [1, 56, 60, 8, 37, 5, 39, 9, 8, 38, 169, 81, 5341, 3, 5341, 82, 7, 69, 287, 2], [1, 50, 26, 5, 8, 38, 326, 81, 2004, 9, 917, 447, 6, 1211, 3, 14, 47, 8, 48, 2], [1, 56, 60, 8, 37, 5, 21, 8, 1135, 475, 28, 489, 5, 39, 9, 8, 38, 2938, 156, 2], [1, 56, 60, 8, 37, 5, 39, 9, 8, 38, 59, 81, 24, 3, 24, 129, 7, 651, 22, 2], [1, 56, 60, 8, 37, 5, 21, 8, 48, 643, 5, 39, 9, 8, 753, 2571, 3, 17, 753, 2], [1, 56, 60, 8, 37, 5, 21, 8, 216, 755, 5, 39, 9, 8, 2010, 38, 59, 81, 24, 2], [1, 56, 60, 8, 37, 5, 21, 8, 1109, 755, 5, 39, 249, 8, 38, 91, 81, 119, 3, 2], [1, 56, 60, 8, 37, 5, 39, 9, 8, 964, 38, 142, 81, 149, 3, 149, 82, 7, 286, 2], [1, 50, 26, 5, 8, 287, 887, 81, 119, 70, 34, 8, 630, 21, 25, 260, 169, 3, 14, 2]]\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(stories, sp, max_length):\n",
    "    inputs, labels = [], []\n",
    "    bos_id, eos_id = sp.bos_id(), sp.eos_id()\n",
    "    \n",
    "    for story in stories:\n",
    "        # Tokenize the story and truncate if necessary\n",
    "        tokens = sp.encode(story, out_type=int)[:max_length - 2]\n",
    "\n",
    "        # Prepend BOS and append EOS token IDs\n",
    "        input_ids = [bos_id] + tokens + [eos_id]\n",
    "        label_ids = [bos_id] + tokens + [eos_id]  # Adjusted to ensure labels also start with bos_id and end with eos_id\n",
    "\n",
    "        # Ensure the final lists are of max_length\n",
    "        # This might already be ensured by previous steps, but double-checking to align with the assertion requirements\n",
    "        input_ids = (input_ids + [eos_id] * max_length)[:max_length]  # Padding with eos_id if necessary, though this should be rare given earlier truncation\n",
    "        label_ids = (label_ids + [eos_id] * max_length)[:max_length]\n",
    "\n",
    "        # Assertions to ensure each sequence meets the specified criteria\n",
    "        assert len(input_ids) == max_length, f\"Input sequence length does not match max_length. Length: {len(input_ids)}\"\n",
    "        assert len(label_ids) == max_length, f\"Label sequence length does not match max_length. Length: {len(label_ids)}\"\n",
    "        assert input_ids[0] == bos_id, \"Input sequence does not start with bos_id.\"\n",
    "        assert label_ids[0] == bos_id, \"Label sequence does not start with bos_id.\"\n",
    "        assert input_ids[-1] == eos_id, \"Input sequence does not end with eos_id.\"\n",
    "        assert label_ids[-1] == eos_id, \"Label sequence does not end with eos_id.\"\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def assert_max_length(data, max_length):\n",
    "    for entry in data:\n",
    "        # Each entry should not exceed max_length tokens\n",
    "        assert len(entry) <= max_length, f\"Entry exceeds max_length of {max_length} tokens.\"\n",
    "\n",
    "\n",
    "train_inputs, train_labels = prepare_data(train_stories, sp, story_token_max_length)\n",
    "assert(len(train_inputs) == len(train_stories))\n",
    "assert_max_length(train_inputs, story_token_max_length)\n",
    "assert_max_length(train_labels, story_token_max_length)\n",
    "\n",
    "valid_inputs, valid_labels = prepare_data(valid_stories, sp, story_token_max_length)\n",
    "assert(len(valid_inputs) == len(valid_stories))\n",
    "assert_max_length(valid_inputs, story_token_max_length)\n",
    "assert_max_length(valid_labels, story_token_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Assuming `train_inputs` and `train_labels` are your processed datasets\n",
    "train_dataset = TinyStoriesDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads = heads, dropout = dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion* embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, add_norm_x, src_mask):\n",
    "        attention_output = self.attention(add_norm_x, add_norm_x, add_norm_x, attn_mask=src_mask)\n",
    "        add_norm_x = self.dropout(self.norm1(attention_output + add_norm_x))\n",
    "        forward = self.feed_forward(add_norm_x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_size, heads, forward_expansion, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fully_connected_layer_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        out = self.fully_connected_layer_out(x)\n",
    "\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerstories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
