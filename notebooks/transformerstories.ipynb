{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan's todo list for tomorrow:\n",
    "\n",
    "* Setup ML Flow for testing\n",
    "* Look into whether Argmax is not the right choice\n",
    "* Save progress after each epoch\n",
    "* Train on bigger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0a - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl (24.2 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, urllib3, tqdm, pyyaml, pyarrow-hotfix, pyarrow, multidict, idna, fsspec, frozenlist, dill, charset-normalizer, certifi, attrs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 idna-3.6 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 requests-2.31.0 sentencepiece-0.2.0 tqdm-4.66.2 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install datasets sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0b - Import module dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on: cpu\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_output_dir = 'sentencepiece_models'\n",
    "sentencepiece_corpus_filename = f\"tiny_stories_texts.txt\"\n",
    "sentencepiece_model_prefix = os.path.join(sentencepiece_output_dir, 'tiny_stories_spm_sampled')\n",
    "\n",
    "story_token_max_length = 20\n",
    "\n",
    "use_small_dataset = True\n",
    "small_data_set_size = 1000\n",
    "\n",
    "vocabulary_size = 8000\n",
    "\n",
    "embedding_size = 256\n",
    "num_decoder_layers = 6\n",
    "num_heads = 8\n",
    "forward_layer_expansion = 4\n",
    "dropout = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running models on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Load datasets\n",
    "\n",
    "Read the tiny stories data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using small datasets\n",
      "Training stories set size: 1000\n",
      "Valisation stories set size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load the Tiny Stories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "\n",
    "train_stories = train_dataset['text']\n",
    "valid_stories = valid_dataset['text']\n",
    "\n",
    "if use_small_dataset is True:\n",
    "    print(\"Using small datasets\")\n",
    "    train_stories = train_stories[:small_data_set_size]\n",
    "    valid_stories = valid_stories[:small_data_set_size]\n",
    "\n",
    "print(f\"Training stories set size: {len(train_stories)}\")\n",
    "print(f\"Valisation stories set size: {len(valid_stories)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all of the data set and export it to a text file for training of the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the files\n",
    "if not os.path.exists(sentencepiece_output_dir):\n",
    "    os.makedirs(sentencepiece_output_dir)\n",
    "\n",
    "# Save all texts to a single file in the specified directory, one story per line\n",
    "sentencepiece_corpus_file_path = os.path.join(sentencepiece_output_dir, sentencepiece_corpus_filename)\n",
    "\n",
    "\n",
    "# Combine texts from training and validation sets\n",
    "all_texts = train_dataset['text'] + valid_dataset['text']\n",
    "\n",
    "random.shuffle(all_texts)\n",
    "\n",
    "# Sample a smaller subset of the dataset, e.g., 10% of the data\n",
    "sample_size = int(0.1 * len(all_texts))\n",
    "sampled_text = all_texts[:sample_size]\n",
    "\n",
    "# Save all texts to a single file, one story per line\n",
    "with open(sentencepiece_corpus_file_path, 'w', encoding='utf-8') as f:\n",
    "    for story in sampled_text:\n",
    "        f.write(story + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next generate the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sentencepiece_models/tiny_stories_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_models/tiny_stories_spm_sampled\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sentencepiece_models/tiny_stories_texts.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1153431), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1153431 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=190919446\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.959% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=56\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99959\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1152172 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=115267307\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 72862 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1152172\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 106169\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 106169 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35342 obj=9.43982 num_tokens=251623 num_tokens/piece=7.11966\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25529 obj=7.2812 num_tokens=253909 num_tokens/piece=9.9459\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19146 obj=7.20603 num_tokens=260995 num_tokens/piece=13.6318\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19142 obj=7.20224 num_tokens=261075 num_tokens/piece=13.6389\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14356 obj=7.20761 num_tokens=278377 num_tokens/piece=19.391\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14356 obj=7.20641 num_tokens=278388 num_tokens/piece=19.3918\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10767 obj=7.21658 num_tokens=300011 num_tokens/piece=27.8639\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10767 obj=7.21443 num_tokens=299951 num_tokens/piece=27.8584\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=7.22739 num_tokens=315917 num_tokens/piece=35.8997\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=7.22539 num_tokens=315871 num_tokens/piece=35.8944\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sentencepiece_models/tiny_stories_spm_sampled.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sentencepiece_models/tiny_stories_spm_sampled.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=sentencepiece_corpus_file_path, model_prefix=sentencepiece_model_prefix, vocab_size=vocabulary_size, character_coverage=0.9995, model_type='unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next initialize the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model_path = f\"{sentencepiece_model_prefix}.model\"\n",
    "sp = spm.SentencePieceProcessor(model_file=spm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Generate the input data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(stories, sp, max_length):\n",
    "    inputs, labels = [], []\n",
    "    bos_id, eos_id = sp.bos_id(), sp.eos_id()\n",
    "    \n",
    "    for story in stories:\n",
    "        # Tokenize the story and truncate if necessary\n",
    "        tokens = sp.encode(story, out_type=int)[:max_length - 2]\n",
    "\n",
    "        # Prepend BOS and append EOS token IDs\n",
    "        input_ids = [bos_id] + tokens + [eos_id]\n",
    "        label_ids = [bos_id] + tokens + [eos_id]  # Adjusted to ensure labels also start with bos_id and end with eos_id\n",
    "\n",
    "        # Ensure the final lists are of max_length\n",
    "        # This might already be ensured by previous steps, but double-checking to align with the assertion requirements\n",
    "        input_ids = (input_ids + [eos_id] * max_length)[:max_length]  # Padding with eos_id if necessary, though this should be rare given earlier truncation\n",
    "        label_ids = (label_ids + [eos_id] * max_length)[:max_length]\n",
    "\n",
    "        # Assertions to ensure each sequence meets the specified criteria\n",
    "        assert len(input_ids) == max_length, f\"Input sequence length does not match max_length. Length: {len(input_ids)}\"\n",
    "        assert len(label_ids) == max_length, f\"Label sequence length does not match max_length. Length: {len(label_ids)}\"\n",
    "        assert input_ids[0] == bos_id, \"Input sequence does not start with bos_id.\"\n",
    "        assert label_ids[0] == bos_id, \"Label sequence does not start with bos_id.\"\n",
    "        assert input_ids[-1] == eos_id, \"Input sequence does not end with eos_id.\"\n",
    "        assert label_ids[-1] == eos_id, \"Label sequence does not end with eos_id.\"\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def assert_max_length(data, max_length):\n",
    "    for entry in data:\n",
    "        # Each entry should not exceed max_length tokens\n",
    "        assert len(entry) <= max_length, f\"Entry exceeds max_length of {max_length} tokens.\"\n",
    "\n",
    "\n",
    "train_inputs, train_labels = prepare_data(train_stories, sp, story_token_max_length)\n",
    "assert(len(train_inputs) == len(train_stories))\n",
    "assert_max_length(train_inputs, story_token_max_length)\n",
    "assert_max_length(train_labels, story_token_max_length)\n",
    "\n",
    "valid_inputs, valid_labels = prepare_data(valid_stories, sp, story_token_max_length)\n",
    "assert(len(valid_inputs) == len(valid_stories))\n",
    "assert_max_length(valid_inputs, story_token_max_length)\n",
    "assert_max_length(valid_labels, story_token_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads = heads, dropout = dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion* embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        attention_output, _ = self.attention(x, x, x, attn_mask=src_mask)\n",
    "        x = self.dropout(self.norm1(attention_output + x))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_size, heads, forward_expansion, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fully_connected_layer_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        out = self.fully_connected_layer_out(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.0090, Validation Loss: 1.6883\n",
      "Epoch 2, Train Loss: 1.2969, Validation Loss: 0.8729\n",
      "Epoch 3, Train Loss: 0.6058, Validation Loss: 0.6022\n",
      "Epoch 4, Train Loss: 0.3016, Validation Loss: 0.4851\n",
      "Epoch 5, Train Loss: 0.1479, Validation Loss: 0.4347\n",
      "Epoch 6, Train Loss: 0.0652, Validation Loss: 0.4084\n",
      "Epoch 7, Train Loss: 0.0254, Validation Loss: 0.3936\n",
      "Epoch 8, Train Loss: 0.0128, Validation Loss: 0.3901\n",
      "Epoch 9, Train Loss: 0.0085, Validation Loss: 0.3891\n",
      "Epoch 10, Train Loss: 0.0064, Validation Loss: 0.3890\n",
      "Epoch 11, Train Loss: 0.0052, Validation Loss: 0.3872\n",
      "Epoch 12, Train Loss: 0.0044, Validation Loss: 0.3865\n",
      "Epoch 13, Train Loss: 0.0038, Validation Loss: 0.3875\n",
      "Epoch 14, Train Loss: 0.0033, Validation Loss: 0.3868\n",
      "Epoch 15, Train Loss: 0.0029, Validation Loss: 0.3857\n",
      "Epoch 16, Train Loss: 0.0025, Validation Loss: 0.3869\n",
      "Epoch 17, Train Loss: 0.0023, Validation Loss: 0.3860\n",
      "Epoch 18, Train Loss: 0.0020, Validation Loss: 0.3872\n",
      "Epoch 19, Train Loss: 0.0019, Validation Loss: 0.3861\n",
      "Epoch 20, Train Loss: 0.0017, Validation Loss: 0.3871\n",
      "Epoch 21, Train Loss: 0.0016, Validation Loss: 0.3868\n",
      "Epoch 22, Train Loss: 0.0014, Validation Loss: 0.3867\n",
      "Epoch 23, Train Loss: 0.0013, Validation Loss: 0.3869\n",
      "Epoch 24, Train Loss: 0.0013, Validation Loss: 0.3868\n",
      "Epoch 25, Train Loss: 0.0012, Validation Loss: 0.3869\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoder(\n",
    "    vocab_size = vocabulary_size,\n",
    "    embed_size = embedding_size,\n",
    "    num_layers = num_decoder_layers,\n",
    "    heads = num_heads,\n",
    "    device = device,\n",
    "    forward_expansion = forward_layer_expansion,\n",
    "    dropout = dropout,\n",
    "    max_length = story_token_max_length\n",
    ").to(device)\n",
    "\n",
    "# Assuming `train_inputs` and `train_labels` are your processed datasets\n",
    "train_dataset_processed = TinyStoriesDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset_processed, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset_processed = TinyStoriesDataset(valid_inputs, valid_labels)\n",
    "validation_loader = DataLoader(validation_dataset_processed, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, src_mask=None)\n",
    "            loss = criterion(outputs.transpose(1,2), labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(loader)\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, src_mask = None)\n",
    "            loss = criterion(outputs.transpose(1,2), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = validate(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, validation_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - inference methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  17, 1544,  628]])\n",
      "Shape: torch.Size([1, 3])\n",
      "Input IDs: tensor([[  17, 1544,  628,   60]])\n",
      "Shape: torch.Size([1, 4])\n",
      "Input IDs: tensor([[  17, 1544,  628,   60,   60]])\n",
      "Shape: torch.Size([1, 5])\n",
      "Input IDs: tensor([[  17, 1544,  628,   60,   60,   60]])\n",
      "Shape: torch.Size([1, 6])\n",
      "Stopping early due to repeated token (60) detected 3 times in a row.\n",
      "The ancient castle upon upon upon\n"
     ]
    }
   ],
   "source": [
    "repetition_threshold = 3\n",
    "\n",
    "def generate_text_simple(model, start_prompt, sp, device, max_length):\n",
    "    model.eval()\n",
    "    words = start_prompt.split()\n",
    "    token_ids = sp.encode(start_prompt, out_type=int)\n",
    "\n",
    "    if max(token_ids) >= vocabulary_size:\n",
    "        raise ValueError(f\"Token ID {max(token_ids)} exceeds vocab size of {vocabulary_size}\")\n",
    "\n",
    "    consecutive_repetitions = 0\n",
    "    last_token_id = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        input_ids = torch.tensor([token_ids], device=device)\n",
    "\n",
    "        print(f\"Input IDs: {input_ids}\")\n",
    "        print(f\"Shape: {input_ids.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, src_mask=None)\n",
    "            predictions = outputs[:, -1, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1).item()\n",
    "\n",
    "        # Check for consecutive repetitions\n",
    "        if predicted_id == last_token_id:\n",
    "            consecutive_repetitions += 1\n",
    "        else:\n",
    "            consecutive_repetitions = 0  # Reset the counter if the current token is different\n",
    "\n",
    "        last_token_id = predicted_id  # Update the last seen token ID\n",
    "\n",
    "        # Exit if the repetition threshold is reached\n",
    "        if consecutive_repetitions >= repetition_threshold:\n",
    "            print(f\"Stopping early due to repeated token ({predicted_id}) detected {repetition_threshold} times in a row.\")\n",
    "            break\n",
    "\n",
    "        if predicted_id == sp.eos_id():\n",
    "            break\n",
    "\n",
    "        token_ids.append(predicted_id)\n",
    "        generated_word = sp.DecodeIds([predicted_id])\n",
    "        words.append(generated_word)\n",
    "\n",
    "    generated_text = ' '.join(words)\n",
    "    return generated_text\n",
    "\n",
    "generated_text = generate_text_simple(model, \"The ancient castle\", sp, device, story_token_max_length)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerstories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
