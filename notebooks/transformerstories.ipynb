{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0a - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/transformerstories/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-15.0.2-cp312-cp312-macosx_11_0_arm64.whl (24.2 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, urllib3, tqdm, pyyaml, pyarrow-hotfix, pyarrow, multidict, idna, fsspec, frozenlist, dill, charset-normalizer, certifi, attrs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 idna-3.6 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 requests-2.31.0 sentencepiece-0.2.0 tqdm-4.66.2 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install datasets sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0b - Import module dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models on: cpu\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_output_dir = 'sentencepiece_models'\n",
    "sentencepiece_corpus_filename = f\"tiny_stories_texts.txt\"\n",
    "sentencepiece_model_prefix = os.path.join(sentencepiece_output_dir, 'tiny_stories_spm_sampled')\n",
    "\n",
    "story_token_max_length = 20\n",
    "\n",
    "use_small_dataset = True\n",
    "small_data_set_size = 100\n",
    "\n",
    "vocabulary_size = 8000\n",
    "\n",
    "embedding_size = 256\n",
    "num_decoder_layers = 6\n",
    "num_heads = 8\n",
    "forward_layer_expansion = 4\n",
    "dropout = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running models on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0c - Load datasets\n",
    "\n",
    "Read the tiny stories data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using small datasets\n",
      "Training stories set size: 100\n",
      "Valisation stories set size: 100\n"
     ]
    }
   ],
   "source": [
    "# Load the Tiny Stories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "\n",
    "train_stories = train_dataset['text']\n",
    "valid_stories = valid_dataset['text']\n",
    "\n",
    "if use_small_dataset is True:\n",
    "    print(\"Using small datasets\")\n",
    "    train_stories = train_stories[:small_data_set_size]\n",
    "    valid_stories = valid_stories[:small_data_set_size]\n",
    "\n",
    "print(f\"Training stories set size: {len(train_stories)}\")\n",
    "print(f\"Valisation stories set size: {len(valid_stories)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all of the data set and export it to a text file for training of the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the files\n",
    "if not os.path.exists(sentencepiece_output_dir):\n",
    "    os.makedirs(sentencepiece_output_dir)\n",
    "\n",
    "# Save all texts to a single file in the specified directory, one story per line\n",
    "sentencepiece_corpus_file_path = os.path.join(sentencepiece_output_dir, sentencepiece_corpus_filename)\n",
    "\n",
    "\n",
    "# Combine texts from training and validation sets\n",
    "all_texts = train_dataset['text'] + valid_dataset['text']\n",
    "\n",
    "random.shuffle(all_texts)\n",
    "\n",
    "# Sample a smaller subset of the dataset, e.g., 10% of the data\n",
    "sample_size = int(0.1 * len(all_texts))\n",
    "sampled_text = all_texts[:sample_size]\n",
    "\n",
    "# Save all texts to a single file, one story per line\n",
    "with open(sentencepiece_corpus_file_path, 'w', encoding='utf-8') as f:\n",
    "    for story in sampled_text:\n",
    "        f.write(story + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next generate the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sentencepiece_models/tiny_stories_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_models/tiny_stories_spm_sampled\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sentencepiece_models/tiny_stories_texts.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1153945), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1153945 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=191014637\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9594% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=56\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999594\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1152723 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=115258353\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 73811 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1152723\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 106315\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 106315 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35500 obj=9.44596 num_tokens=253420 num_tokens/piece=7.13859\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25751 obj=7.28099 num_tokens=255059 num_tokens/piece=9.90482\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19311 obj=7.20648 num_tokens=261671 num_tokens/piece=13.5504\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19308 obj=7.20255 num_tokens=261758 num_tokens/piece=13.557\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14480 obj=7.20805 num_tokens=279075 num_tokens/piece=19.2731\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14480 obj=7.20694 num_tokens=279056 num_tokens/piece=19.2718\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10860 obj=7.21705 num_tokens=300656 num_tokens/piece=27.6847\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10860 obj=7.21508 num_tokens=300618 num_tokens/piece=27.6812\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=7.22945 num_tokens=317129 num_tokens/piece=36.0374\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=7.22745 num_tokens=317102 num_tokens/piece=36.0343\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sentencepiece_models/tiny_stories_spm_sampled.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sentencepiece_models/tiny_stories_spm_sampled.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=sentencepiece_corpus_file_path, model_prefix=sentencepiece_model_prefix, vocab_size=vocabulary_size, character_coverage=0.9995, model_type='unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next initialize the sentence piece model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model_path = f\"{sentencepiece_model_prefix}.model\"\n",
    "sp = spm.SentencePieceProcessor(model_file=spm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Generate the input data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(stories, sp, max_length):\n",
    "    inputs, labels = [], []\n",
    "    bos_id, eos_id = sp.bos_id(), sp.eos_id()\n",
    "    \n",
    "    for story in stories:\n",
    "        # Tokenize the story and truncate if necessary\n",
    "        tokens = sp.encode(story, out_type=int)[:max_length - 2]\n",
    "\n",
    "        # Prepend BOS and append EOS token IDs\n",
    "        input_ids = [bos_id] + tokens + [eos_id]\n",
    "        label_ids = [bos_id] + tokens + [eos_id]  # Adjusted to ensure labels also start with bos_id and end with eos_id\n",
    "\n",
    "        # Ensure the final lists are of max_length\n",
    "        # This might already be ensured by previous steps, but double-checking to align with the assertion requirements\n",
    "        input_ids = (input_ids + [eos_id] * max_length)[:max_length]  # Padding with eos_id if necessary, though this should be rare given earlier truncation\n",
    "        label_ids = (label_ids + [eos_id] * max_length)[:max_length]\n",
    "\n",
    "        # Assertions to ensure each sequence meets the specified criteria\n",
    "        assert len(input_ids) == max_length, f\"Input sequence length does not match max_length. Length: {len(input_ids)}\"\n",
    "        assert len(label_ids) == max_length, f\"Label sequence length does not match max_length. Length: {len(label_ids)}\"\n",
    "        assert input_ids[0] == bos_id, \"Input sequence does not start with bos_id.\"\n",
    "        assert label_ids[0] == bos_id, \"Label sequence does not start with bos_id.\"\n",
    "        assert input_ids[-1] == eos_id, \"Input sequence does not end with eos_id.\"\n",
    "        assert label_ids[-1] == eos_id, \"Label sequence does not end with eos_id.\"\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def assert_max_length(data, max_length):\n",
    "    for entry in data:\n",
    "        # Each entry should not exceed max_length tokens\n",
    "        assert len(entry) <= max_length, f\"Entry exceeds max_length of {max_length} tokens.\"\n",
    "\n",
    "\n",
    "train_inputs, train_labels = prepare_data(train_stories, sp, story_token_max_length)\n",
    "assert(len(train_inputs) == len(train_stories))\n",
    "assert_max_length(train_inputs, story_token_max_length)\n",
    "assert_max_length(train_labels, story_token_max_length)\n",
    "\n",
    "valid_inputs, valid_labels = prepare_data(valid_stories, sp, story_token_max_length)\n",
    "assert(len(valid_inputs) == len(valid_stories))\n",
    "assert_max_length(valid_inputs, story_token_max_length)\n",
    "assert_max_length(valid_labels, story_token_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads = heads, dropout = dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion* embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        attention_output, _ = self.attention(x, x, x, attn_mask=src_mask)\n",
    "        x = self.dropout(self.norm1(attention_output + x))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_size, heads, forward_expansion, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fully_connected_layer_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        out = self.fully_connected_layer_out(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.2935, Validation Loss: 4.4145\n",
      "Epoch 2, Train Loss: 4.4164, Validation Loss: 3.1123\n",
      "Epoch 3, Train Loss: 3.2259, Validation Loss: 2.4775\n",
      "Epoch 4, Train Loss: 2.5634, Validation Loss: 1.8930\n",
      "Epoch 5, Train Loss: 1.9534, Validation Loss: 1.6019\n",
      "Epoch 6, Train Loss: 1.5956, Validation Loss: 1.3947\n",
      "Epoch 7, Train Loss: 1.1596, Validation Loss: 1.2515\n",
      "Epoch 8, Train Loss: 0.9121, Validation Loss: 1.1474\n",
      "Epoch 9, Train Loss: 0.7144, Validation Loss: 1.0663\n",
      "Epoch 10, Train Loss: 0.6498, Validation Loss: 1.0226\n",
      "Epoch 11, Train Loss: 0.3975, Validation Loss: 0.9799\n",
      "Epoch 12, Train Loss: 0.3428, Validation Loss: 0.9517\n",
      "Epoch 13, Train Loss: 0.2278, Validation Loss: 0.9381\n",
      "Epoch 14, Train Loss: 0.1916, Validation Loss: 0.9047\n",
      "Epoch 15, Train Loss: 0.1129, Validation Loss: 0.8962\n",
      "Epoch 16, Train Loss: 0.0824, Validation Loss: 0.8926\n",
      "Epoch 17, Train Loss: 0.0631, Validation Loss: 0.8841\n",
      "Epoch 18, Train Loss: 0.0490, Validation Loss: 0.8789\n",
      "Epoch 19, Train Loss: 0.0380, Validation Loss: 0.8704\n",
      "Epoch 20, Train Loss: 0.0331, Validation Loss: 0.8697\n",
      "Epoch 21, Train Loss: 0.0369, Validation Loss: 0.8700\n",
      "Epoch 22, Train Loss: 0.0296, Validation Loss: 0.8653\n",
      "Epoch 23, Train Loss: 0.0217, Validation Loss: 0.8614\n",
      "Epoch 24, Train Loss: 0.0211, Validation Loss: 0.8651\n",
      "Epoch 25, Train Loss: 0.0228, Validation Loss: 0.8652\n",
      "Epoch 26, Train Loss: 0.0148, Validation Loss: 0.8591\n",
      "Epoch 27, Train Loss: 0.0140, Validation Loss: 0.8557\n",
      "Epoch 28, Train Loss: 0.0121, Validation Loss: 0.8566\n",
      "Epoch 29, Train Loss: 0.0115, Validation Loss: 0.8581\n",
      "Epoch 30, Train Loss: 0.0135, Validation Loss: 0.8576\n",
      "Epoch 31, Train Loss: 0.0119, Validation Loss: 0.8561\n",
      "Epoch 32, Train Loss: 0.0108, Validation Loss: 0.8508\n",
      "Epoch 33, Train Loss: 0.0089, Validation Loss: 0.8483\n",
      "Epoch 34, Train Loss: 0.0084, Validation Loss: 0.8492\n",
      "Epoch 35, Train Loss: 0.0080, Validation Loss: 0.8519\n",
      "Epoch 36, Train Loss: 0.0089, Validation Loss: 0.8546\n",
      "Epoch 37, Train Loss: 0.0071, Validation Loss: 0.8551\n",
      "Epoch 38, Train Loss: 0.0072, Validation Loss: 0.8529\n",
      "Epoch 39, Train Loss: 0.0062, Validation Loss: 0.8504\n",
      "Epoch 40, Train Loss: 0.0081, Validation Loss: 0.8504\n",
      "Epoch 41, Train Loss: 0.0058, Validation Loss: 0.8532\n",
      "Epoch 42, Train Loss: 0.0073, Validation Loss: 0.8540\n",
      "Epoch 43, Train Loss: 0.0060, Validation Loss: 0.8517\n",
      "Epoch 44, Train Loss: 0.0079, Validation Loss: 0.8483\n",
      "Epoch 45, Train Loss: 0.0059, Validation Loss: 0.8446\n",
      "Epoch 46, Train Loss: 0.0064, Validation Loss: 0.8464\n",
      "Epoch 47, Train Loss: 0.0048, Validation Loss: 0.8501\n",
      "Epoch 48, Train Loss: 0.0072, Validation Loss: 0.8529\n",
      "Epoch 49, Train Loss: 0.0055, Validation Loss: 0.8532\n",
      "Epoch 50, Train Loss: 0.0051, Validation Loss: 0.8517\n",
      "Epoch 51, Train Loss: 0.0053, Validation Loss: 0.8497\n",
      "Epoch 52, Train Loss: 0.0051, Validation Loss: 0.8482\n",
      "Epoch 53, Train Loss: 0.0045, Validation Loss: 0.8482\n",
      "Epoch 54, Train Loss: 0.0043, Validation Loss: 0.8485\n",
      "Epoch 55, Train Loss: 0.0040, Validation Loss: 0.8496\n",
      "Epoch 56, Train Loss: 0.0037, Validation Loss: 0.8509\n",
      "Epoch 57, Train Loss: 0.0042, Validation Loss: 0.8514\n",
      "Epoch 58, Train Loss: 0.0037, Validation Loss: 0.8515\n",
      "Epoch 59, Train Loss: 0.0045, Validation Loss: 0.8508\n",
      "Epoch 60, Train Loss: 0.0041, Validation Loss: 0.8496\n",
      "Epoch 61, Train Loss: 0.0045, Validation Loss: 0.8492\n",
      "Epoch 62, Train Loss: 0.0034, Validation Loss: 0.8495\n",
      "Epoch 63, Train Loss: 0.0037, Validation Loss: 0.8498\n",
      "Epoch 64, Train Loss: 0.0032, Validation Loss: 0.8494\n",
      "Epoch 65, Train Loss: 0.0032, Validation Loss: 0.8490\n",
      "Epoch 66, Train Loss: 0.0033, Validation Loss: 0.8486\n",
      "Epoch 67, Train Loss: 0.0039, Validation Loss: 0.8485\n",
      "Epoch 68, Train Loss: 0.0029, Validation Loss: 0.8493\n",
      "Epoch 69, Train Loss: 0.0029, Validation Loss: 0.8503\n",
      "Epoch 70, Train Loss: 0.0033, Validation Loss: 0.8505\n",
      "Epoch 71, Train Loss: 0.0037, Validation Loss: 0.8500\n",
      "Epoch 72, Train Loss: 0.0031, Validation Loss: 0.8500\n",
      "Epoch 73, Train Loss: 0.0034, Validation Loss: 0.8492\n",
      "Epoch 74, Train Loss: 0.0029, Validation Loss: 0.8486\n",
      "Epoch 75, Train Loss: 0.0031, Validation Loss: 0.8481\n",
      "Epoch 76, Train Loss: 0.0028, Validation Loss: 0.8475\n",
      "Epoch 77, Train Loss: 0.0029, Validation Loss: 0.8475\n",
      "Epoch 78, Train Loss: 0.0026, Validation Loss: 0.8476\n",
      "Epoch 79, Train Loss: 0.0026, Validation Loss: 0.8482\n",
      "Epoch 80, Train Loss: 0.0025, Validation Loss: 0.8487\n",
      "Epoch 81, Train Loss: 0.0022, Validation Loss: 0.8488\n",
      "Epoch 82, Train Loss: 0.0023, Validation Loss: 0.8491\n",
      "Epoch 83, Train Loss: 0.0021, Validation Loss: 0.8493\n",
      "Epoch 84, Train Loss: 0.0020, Validation Loss: 0.8493\n",
      "Epoch 85, Train Loss: 0.0021, Validation Loss: 0.8490\n",
      "Epoch 86, Train Loss: 0.0020, Validation Loss: 0.8490\n",
      "Epoch 87, Train Loss: 0.0023, Validation Loss: 0.8491\n",
      "Epoch 88, Train Loss: 0.0021, Validation Loss: 0.8497\n",
      "Epoch 89, Train Loss: 0.0021, Validation Loss: 0.8495\n",
      "Epoch 90, Train Loss: 0.0022, Validation Loss: 0.8494\n",
      "Epoch 91, Train Loss: 0.0021, Validation Loss: 0.8493\n",
      "Epoch 92, Train Loss: 0.0021, Validation Loss: 0.8494\n",
      "Epoch 93, Train Loss: 0.0019, Validation Loss: 0.8492\n",
      "Epoch 94, Train Loss: 0.0018, Validation Loss: 0.8489\n",
      "Epoch 95, Train Loss: 0.0018, Validation Loss: 0.8489\n",
      "Epoch 96, Train Loss: 0.0018, Validation Loss: 0.8488\n",
      "Epoch 97, Train Loss: 0.0018, Validation Loss: 0.8484\n",
      "Epoch 98, Train Loss: 0.0018, Validation Loss: 0.8483\n",
      "Epoch 99, Train Loss: 0.0019, Validation Loss: 0.8481\n",
      "Epoch 100, Train Loss: 0.0016, Validation Loss: 0.8484\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoder(\n",
    "    vocab_size = vocabulary_size,\n",
    "    embed_size = embedding_size,\n",
    "    num_layers = num_decoder_layers,\n",
    "    heads = num_heads,\n",
    "    device = device,\n",
    "    forward_expansion = forward_layer_expansion,\n",
    "    dropout = dropout,\n",
    "    max_length = story_token_max_length\n",
    ").to(device)\n",
    "\n",
    "# Assuming `train_inputs` and `train_labels` are your processed datasets\n",
    "train_dataset_processed = TinyStoriesDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset_processed, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset_processed = TinyStoriesDataset(valid_inputs, valid_labels)\n",
    "validation_loader = DataLoader(validation_dataset_processed, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, src_mask=None)\n",
    "            loss = criterion(outputs.transpose(1,2), labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(loader)\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, src_mask = None)\n",
    "            loss = criterion(outputs.transpose(1,2), labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = validate(model, valid_loader, criterion, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, validation_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - inference methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  17, 1534,  604]])\n",
      "Shape: torch.Size([1, 3])\n",
      "Input IDs: tensor([[  17, 1534,  604,    4]])\n",
      "Shape: torch.Size([1, 4])\n",
      "Input IDs: tensor([[  17, 1534,  604,    4,    4]])\n",
      "Shape: torch.Size([1, 5])\n",
      "Input IDs: tensor([[  17, 1534,  604,    4,    4,    4]])\n",
      "Shape: torch.Size([1, 6])\n",
      "Stopping early due to repeated token (4) detected 3 times in a row.\n",
      "The ancient castle and and and\n"
     ]
    }
   ],
   "source": [
    "repetition_threshold = 3\n",
    "\n",
    "def generate_text_simple(model, start_prompt, sp, device, max_length):\n",
    "    model.eval()\n",
    "    words = start_prompt.split()\n",
    "    token_ids = sp.encode(start_prompt, out_type=int)\n",
    "\n",
    "    if max(token_ids) >= vocabulary_size:\n",
    "        raise ValueError(f\"Token ID {max(token_ids)} exceeds vocab size of {vocabulary_size}\")\n",
    "\n",
    "    consecutive_repetitions = 0\n",
    "    last_token_id = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        input_ids = torch.tensor([token_ids], device=device)\n",
    "\n",
    "        print(f\"Input IDs: {input_ids}\")\n",
    "        print(f\"Shape: {input_ids.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, src_mask=None)\n",
    "            predictions = outputs[:, -1, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1).item()\n",
    "\n",
    "        # Check for consecutive repetitions\n",
    "        if predicted_id == last_token_id:\n",
    "            consecutive_repetitions += 1\n",
    "        else:\n",
    "            consecutive_repetitions = 0  # Reset the counter if the current token is different\n",
    "\n",
    "        last_token_id = predicted_id  # Update the last seen token ID\n",
    "\n",
    "        # Exit if the repetition threshold is reached\n",
    "        if consecutive_repetitions >= repetition_threshold:\n",
    "            print(f\"Stopping early due to repeated token ({predicted_id}) detected {repetition_threshold} times in a row.\")\n",
    "            break\n",
    "\n",
    "        if predicted_id == sp.eos_id():\n",
    "            break\n",
    "\n",
    "        token_ids.append(predicted_id)\n",
    "        generated_word = sp.DecodeIds([predicted_id])\n",
    "        words.append(generated_word)\n",
    "\n",
    "    generated_text = ' '.join(words)\n",
    "    return generated_text\n",
    "\n",
    "generated_text = generate_text_simple(model, \"The ancient castle\", sp, device, story_token_max_length)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerstories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
